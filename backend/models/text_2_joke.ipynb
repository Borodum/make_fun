{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec49c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40bd57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../datasets/rJokes/preprocessed.csv'\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "df.head(5)\n",
    "cleaned_df = df.dropna()\n",
    "cleaned_df.head(5)\n",
    "data = pd.DataFrame()\n",
    "data['caption'] = cleaned_df['body'].copy()\n",
    "data['joke'] = cleaned_df['joke'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a6dd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f53a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2164.52 examples/s]\n",
      "c:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/250 00:10 < 10:28, 0.39 it/s, Epoch 0.20/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     74\u001b[39m training_args = TrainingArguments(\n\u001b[32m     75\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./joke_generator\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m     num_train_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m     weight_decay=\u001b[32m0.01\u001b[39m,\n\u001b[32m     83\u001b[39m )\n\u001b[32m     85\u001b[39m trainer = CustomTrainer(\n\u001b[32m     86\u001b[39m     model=model,\n\u001b[32m     87\u001b[39m     args=training_args,\n\u001b[32m     88\u001b[39m     train_dataset=tokenized_dataset,\n\u001b[32m     89\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./joke_generator_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     94\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./joke_generator_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    max_length = 100\n",
    "    inputs = tokenizer(\n",
    "        examples[\"caption\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        examples[\"joke\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "        \"labels\": targets[\"input_ids\"].squeeze()\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        min_seq_length = min(shift_logits.size(1), shift_labels.size(1))\n",
    "        shift_logits = shift_logits[:, :min_seq_length, :].contiguous()\n",
    "        shift_labels = shift_labels[:, :min_seq_length].contiguous()\n",
    "\n",
    "        vocab_size = shift_logits.size(-1)\n",
    "        batch_size, seq_length = shift_labels.size()\n",
    "        shift_labels_one_hot = F.one_hot(shift_labels, num_classes=vocab_size).float()\n",
    "\n",
    "        shift_logits = shift_logits.view(-1, vocab_size)\n",
    "        shift_labels_one_hot = shift_labels_one_hot.view(-1, vocab_size)\n",
    "\n",
    "\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(shift_logits, shift_labels_one_hot)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./joke_generator\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./joke_generator_model\")\n",
    "tokenizer.save_pretrained(\"./joke_generator_model\")\n",
    "\n",
    "def generate_joke(caption):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True, max_length=100).to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=400,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    joke = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return joke\n",
    "\n",
    "caption = \"A cat in a hat\"\n",
    "joke = generate_joke(caption)\n",
    "print(f\"Caption: {caption}\")\n",
    "print(f\"Joke: {joke}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 543407fe-08f5-4674-989f-9527c8fc5f7e)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2514.19 examples/s]\n",
      "c:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Borodum\\AppData\\Local\\Temp\\ipykernel_11712\\3097956661.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ImprovedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = ImprovedTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:\n",
      "trainable params: 40,958,208 || all params: 165,399,552 || trainable%: 24.7632\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 26:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.510000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Borodum\\Desktop\\mlLabs\\PMLDL\\team_project\\make_fun\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Joke Generation:\n",
      "\n",
      "Caption: A cat in a hat\n",
      "Generated Joke: ðŒˆï¿½ï¿½ï¿½ï¿½ð Œ€ð“Œð¡Œ‚ð™Œ‘ð’Œ“ð”Œ’ð‘Œ”ðšŒ™ð›Œ›ðŒðœŒžðžŒœðŸŒ ð¤ŒŸð¥Œ¡ð£Œ¤ð¦Œ¥ðªŒ¨ð©Œ¬ð¨Œ«ð¬Œªð­Œ­ï¿½\n",
      "\n",
      "--------------------------------------------------\n",
      "Caption: A programmer at a coffee shop\n",
      "Generated Joke: ðŒ€ð’Œ‚ð™Œð¤Œ‘ð Œ“ð¡Œ—ð“Œ˜ð£Œ™ï¿½ð¦Œ›ð›Œ”ðŒï¿½ð—¼ðœŒ¡ï¿½ð”žð‘Œ ð•Œ¥ð˜Œ­ð­é’ðšŒŸðŸŒ°ð¢Œ£ï¿½\n",
      "\n",
      "--------------------------------------------------\n",
      "Caption: A banana wearing sunglasses\n",
      "Generated Joke: ðŒ€ð’Œ‚ð‘Œƒð“Œð”ï¿½ï¿½ð ‘“ð¡Œ«ð™Œ¬ï¿½ð¤­ð£Œ¨ï¿½ð›¹ï¿½ð¦ºï¿½ï¿½ðœŒ±ï¿½ï¿½ð¿´ð±±—ð­¼ð´¾ï¿½\n",
      "\n",
      "--------------------------------------------------\n",
      "Caption: Two robots in love\n",
      "Generated Joke: One of them is a woman ðŒï¿½ð’Œ·ð™Œ±ð Œð¡Œ¬ð˜Œ­ð¦Œ²ð“Œ´ï¿½ð”Œ˜ð–Œ¨ð‘Œ®ð›Œ³ðœŒ¼ðžŒ½ðŸŒ¾ð£Œ¹ð¤Œ¿ð¥Œºð¨Œ¸ï¿½ðªŒ°ð©¦ï¿½\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    Trainer, TrainingArguments, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"[CAPTION]\", \"[JOKE]\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def format_prompt(caption: str, joke: str = None) -> str:\n",
    "    \"\"\"Format the input for causal language modeling\"\"\"\n",
    "    prompt = f\"[CAPTION] {caption} [JOKE]\"\n",
    "    if joke:\n",
    "        prompt += f\" {joke}{tokenizer.eos_token}\"\n",
    "    return prompt\n",
    "\n",
    "def tokenize_function(examples: Dict) -> Dict:\n",
    "    \"\"\"Improved tokenization with proper formatting\"\"\"\n",
    "    \n",
    "    formatted_texts = [format_prompt(caption, joke) \n",
    "                      for caption, joke in zip(examples[\"caption\"], examples[\"joke\"])]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        formatted_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "class ImprovedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Use standard cross-entropy loss for language modeling\"\"\"\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./joke_generator\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon=1e-8,\n",
    "    max_grad_norm=1.0,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ImprovedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./joke_generator_model\")\n",
    "tokenizer.save_pretrained(\"./joke_generator_model\")\n",
    "\n",
    "\n",
    "def generate_joke(caption: str, max_joke_length: int = 100) -> str:\n",
    "    \"\"\"Improved generation with better parameters\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    prompt = format_prompt(caption)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(inputs[\"input_ids\"][0]) + max_joke_length,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    if \"[JOKE]\" in generated_text:\n",
    "        joke_start = generated_text.find(\"[JOKE]\") + 6\n",
    "        joke = generated_text[joke_start:].split(tokenizer.eos_token)[0].strip()\n",
    "    else:\n",
    "        joke = generated_text[len(prompt):].split(tokenizer.eos_token)[0].strip()\n",
    "    \n",
    "    return joke\n",
    "\n",
    "\n",
    "def test_joke_generation():\n",
    "    \"\"\"Test the model with various captions\"\"\"\n",
    "    test_captions = [\n",
    "        \"A cat in a hat\",\n",
    "        \"A programmer at a coffee shop\", \n",
    "        \"A banana wearing sunglasses\",\n",
    "        \"Two robots in love\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Testing Joke Generation:\\n\")\n",
    "    for caption in test_captions:\n",
    "        try:\n",
    "            joke = generate_joke(caption)\n",
    "            print(f\"Caption: {caption}\")\n",
    "            print(f\"Generated Joke: {joke}\\n\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating joke for '{caption}': {e}\")\n",
    "\n",
    "test_joke_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cbd501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: Two gay guys are lying on their bed.  The first guy asks the second:\n",
      "\n",
      "\"Do you hate me?\"\n",
      "\n",
      "Second Guy replies , \"No...\"\n",
      "\n",
      "\"Do you find me annoying?\" - \"No\"\n",
      "\n",
      "\"Do you think I'm ugly?\" - \"No\"\n",
      "\n",
      "\"Is there some one else?\" - \"No\"\n",
      "\n",
      "\"Do you want me to leave? \" \n",
      "\n",
      "\"Don't you want to have sex with me?\"\n",
      "Second guy says \"I do! But why are you asking me these stupid questions?\"\n",
      "\n",
      "First Guy: \"Then why are you facing towards me!?\"\n",
      "\n",
      "--------------------------------------------------\n",
      "Second Guy : \"Because you're a gay guy!\"  Kenyan guy replies \"You're not gay! You're just a guy who \n",
      "likes to lie on his bed!   \" Do you like me?  I hate you! No, I don't! I find you annoying!\" Second \n",
      "guy : ( \"Why do you ask me this stupid question?\" ) Kenyan dude replies :  Nigerian guy:  He says he\n",
      " likes me because he's gay, but he doesn't like to talk about\n"
     ]
    }
   ],
   "source": [
    "caption = data['caption'].values[3]\n",
    "joke = generate_joke(caption)\n",
    "print(f\"Caption: {caption}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(0, len(joke), 100):\n",
    "    print(joke[i:i+100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
