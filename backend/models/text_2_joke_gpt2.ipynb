{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "\n",
    "file1 = 'combined-jokes/reddit_jokes_output.csv'\n",
    "file2 = 'combined-jokes/shortjokes.csv'\n",
    "\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"unique_jokes_df = combined_df.drop_duplicates(subset=['Joke'])\n",
    "\n",
    "unique_jokes_df = unique_jokes_df.reset_index(drop=True)\n",
    "unique_jokes_df['ID'] = range(1, len(unique_jokes_df) + 1)\n",
    "\n",
    "columns = ['ID'] + [col for col in unique_jokes_df.columns if col != 'ID']\n",
    "unique_jokes_df = unique_jokes_df[columns]\n",
    "\n",
    "output_file = './combined_jokes.csv'\n",
    "unique_jokes_df.to_csv(output_file, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"{output_file}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ac34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"file_path = \"./combined_jokes.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.info()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import re\n",
    "CONTRACTIONS = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"here's\": \"here is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"how's\": \"how is\",\n",
    "}\n",
    "sensitive_words = [\n",
    "    \"sex\", \"gender\", \"womanizer\", \"feminist\", \"chauvinist\", \"misogynist\",\n",
    "    \"misandrist\", \"patriarchy\", \"matriarchy\",\n",
    "    \n",
    "    \"racism\", \"racist\", \"black\", \"white\", \"asian\", \"african\", \"indian\",\n",
    "    \"nigger\", \"cracker\", \"slur\", \"latino\", \"hispanic\", \"jew\", \"antisemitic\",\n",
    "    \"arab\", \"middle eastern\", \"xenophobia\", \"immigrant\",\n",
    "    \n",
    "    \"penis\",\"fuck\", \"vagina\", \"breast\", \"boobs\", \"dick\", \"cock\", \"pussy\", \"butt\",\n",
    "    \"anus\", \"testicle\", \"scrotum\", \"nipple\", \"semen\", \"sperm\", \"condom\",\n",
    "    \"masturbate\", \"ejaculate\", \"rape\", \"molest\", \"abuse\", \"hooker\", \n",
    "    \"prostitute\", \"whore\", \"slut\", \"porn\", \"pornography\", \"fetish\",\n",
    "    \"orgy\", \"orgasm\", \"clitoris\", \"labia\", \"erection\", \"virginity\",\n",
    "    \n",
    "    \"dumb\", \"idiot\", \"moron\", \"retard\", \"cripple\",\n",
    "    \"gay\", \"lesbian\", \"homosexual\", \"bisexual\", \"transgender\", \n",
    "    \"queer\", \"homo\", \"tranny\", \"dyke\", \"fag\", \"pervert\", \n",
    "    \"harass\", \"molestation\", \"pedophile\", \"incest\"\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d70576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clean_text(text):\n",
    "    text = text.replace('\"', '')\n",
    "    text = re.sub(r\"&\\w+;\", \"\", text)\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\.{3,}\", \".\", text)\n",
    "    text = re.sub(r\"\\!{2,}\", \"!\", text)\n",
    "    text = re.sub(r\"\\?{2,}\", \"?\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def expand_contractions(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in CONTRACTIONS.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: CONTRACTIONS[x.group()], text)\n",
    "\n",
    "def contains_sensitive_words(text):\n",
    "    return any(word in text for word in sensitive_words)\n",
    "\n",
    "#def preprocess_text(text):\n",
    "#    tokens = word_tokenize(text)\n",
    "#    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#    return tokens\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "008a9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model and tokenizer...\n",
      "Model setup complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"./work/fine_tuned_gpt2_small_kaggle_final_1221/\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Loading fine-tuned model and tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Wrap with DataParallel if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(\"Model setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5366df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(prompt, model, tokenizer, device, max_length=100, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,  \n",
    "            pad_token_id=tokenizer.pad_token_id,  \n",
    "            eos_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "\n",
    "    joke = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return joke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "621f81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Why did the chicken cross the road?\n",
      "Generated Joke: Why did the chicken cross the road? it was a lame excuse.\n",
      "--------------------------------------------------\n",
      "Prompt: What’s orange and sounds like a parrot?\n",
      "Generated Joke: What’s orange and sounds like a parrot? an elephant!\n",
      "--------------------------------------------------\n",
      "Prompt: Why was the math book sad?\n",
      "Generated Joke: Why was the math book sad? it had too many problems.\n",
      "--------------------------------------------------\n",
      "Prompt: Why don’t skeletons fight each other?\n",
      "Generated Joke: Why don’t skeletons fight each other? because they have no guts.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Why did the chicken cross the road?\",\n",
    "    \"What’s orange and sounds like a parrot?\",\n",
    "    \"Why was the math book sad?\",\n",
    "    \"Why don’t skeletons fight each other?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    joke = generate_joke(prompt, model, tokenizer, device)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Joke: {joke}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
